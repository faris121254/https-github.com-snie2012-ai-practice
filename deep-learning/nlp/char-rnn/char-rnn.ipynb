{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# char-rnn practice following Andrej Karpathy's code example: https://gist.github.com/karpathy/d4dee566867f8291f086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = open('data/input.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "chars = list(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_size, vocab_size = len(data), len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique\n"
     ]
    }
   ],
   "source": [
    "print 'data has %d characters, %d unique' % (data_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "char_to_ix = { ch:i for i, ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# hypterparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01  # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01  # hidden to output\n",
    "bh = np.zeros((hidden_size, 1))  # hidden bias\n",
    "by = np.zeros((vocab_size, 1))  # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs, targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state.\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    \n",
    "    # forward pass\n",
    "    for t in xrange(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t], 0]) # softmax (cross-entropy loss)\n",
    "    \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh ,dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    \n",
    "    for t in reversed(xrange(len(inputs))):\n",
    "        # there is an output for each time step\n",
    "        # recall the different model structures in http://www.deeplearningbook.org/contents/rnn.html, which strcuture does it match?\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y \n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        # unlike non-recurrent neural networks, the same weights and biases are multiplied and added across many time steps,\n",
    "        # if we consider the full detailed form of the function of output and these parameters, the parameters appears many times.\n",
    "        # Therefore, addition has to be performed to calculate the gradients correctly\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw) # the gradient propogated from the next hidden state\n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigte exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample s sequence of integers from the model\n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in xrange(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " in:\n",
      "He hom of bloon wlose. Yfor deadinginge this hee clarie in beselyatenest the an so houne:\n",
      "Am yours ops am th son pull of sols to her reot with or 'est a spornsgrel's, but ev canireds nond Soints:  \n",
      "----\n",
      "iter 0, loss: 104.303868\n",
      "----\n",
      " 'es;A\n",
      "uu\n",
      "br btSre,sets wa re ojtint speses whons sth:\n",
      "\n",
      "CAtaly\n",
      "ssareanbs at char\n",
      "Azbtotr stirept dotocititSsekfelst iotobs anltetipsans BuayorN eans socen eas as fnRhhv;itSre soa hotide, is rs Moedowte \n",
      "----\n",
      "iter 100, loss: 103.831260\n",
      "----\n",
      " t ah, icdy\n",
      "S nsgh?: ggor y\n",
      "Thtey toc! woly tyilre dhlle\n",
      "euk koy, s tive osWdepetotWercoron\n",
      "Tan?\n",
      "\n",
      "O e iste,\n",
      "Otald.\n",
      "Wbu;e onay resauity'ly?\n",
      "Wee wotu sfatsitinyve zig wam ay s tound fa, ptu nemAd ttius g \n",
      "----\n",
      "iter 200, loss: 101.269216\n",
      "----\n",
      " t,\n",
      "As\n",
      "we tamyel\n",
      "MORMeufdau tak upwe w'ra Iof,\n",
      "Fatut thole- thhoy\n",
      "H falrl;\n",
      "\n",
      "COFan houl wupe the th arde y Os tuet Cimye d\n",
      "OMMe thhe tI Iy tha, wod sat infdet, bluld ies:\n",
      "He\n",
      "Hhe tet roit weat mor ettuld \n",
      "----\n",
      "iter 300, loss: 98.345376\n",
      "----\n",
      " mganen pas\n",
      "Epath arislul the kif, Corp maee anll re man akithanlur thoe in thavi's, kinr:\n",
      "Nranre ko coEt ot dat thoer' rut se awndly\n",
      "The alonlrli thet woid thapf cone fos grl pivese\n",
      "izat fec vos the b \n",
      "----\n",
      "iter 400, loss: 95.575399\n",
      "----\n",
      "  we\n",
      "hgeras kove ghapeg; Lhisars ust tule'e th thiutot Lurs.\n",
      "Lhe MI Mo:.\n",
      "TOM\n",
      "Srcird sols at, the ptoos. kiert; ee po lill Otiw ihls urlet the us on, whe wtit gcege emipens atis cus ore,\n",
      "Ouctos my it th \n",
      "----\n",
      "iter 500, loss: 93.024500\n",
      "----\n",
      " s slourly kor worakiandt aes kimd fa,? fewee, alet tphan ron,\n",
      "Youm Sherefeg Wheowr rord pwibs as brel yesesn fousasthLeos lI Ly,\n",
      "ORVE\n",
      "MBBWhals nit wogtotakenst dakfr'd ibd tinos,.\n",
      "Whaly of. doudf meen \n",
      "----\n",
      "iter 600, loss: 90.568120\n",
      "----\n",
      " f wauf curs hout tathis,\n",
      " theounbt buw whoA hte thaled fes bitere fend dan seante'mocanc yirmee rordes\n",
      "Send in montN;.\n",
      "Sound aec OraV, ell yon dhrel'e or rawal cely thor aon dhe oiin nrat f\n",
      "I,..\n",
      "I\n",
      "SES \n",
      "----\n",
      "iter 700, loss: 88.475678\n",
      "----\n",
      " hke enohe.\n",
      "JlI bothobh ye aluey frisnin sas obcoutn sheiks, an my homtour po'dt toth thet:\n",
      "Ocelth rere hes, an shaf fokl tolifey. at obue.\n",
      "undces ir, worbe,,\n",
      "Yoe shid a I  yima ot, fors o bluu thathid \n",
      "----\n",
      "iter 800, loss: 86.339932\n",
      "----\n",
      " \n",
      "\n",
      "Lund ethef,\n",
      "\n",
      "MAESe,\n",
      "\n",
      "Sbillte Heaves at ral;\n",
      "morrde,\n",
      "Waf;s tut mfoh y'Lioresy rron, er,\n",
      " as,\n",
      " dilts wh\n",
      "Not uy, I menw mefge ye. ils.\n",
      "\n",
      "zdat vin. benut,\n",
      "fe steall lens; I an,\n",
      "\n",
      "RIF Yiv uld rourd bod;:\n",
      "P \n",
      "----\n",
      "iter 900, loss: 84.239245\n",
      "----\n",
      " wt an ro seon dvorey?\n",
      "Hite went,\n",
      "T;M on rt;y fou's:\n",
      "\n",
      "Chind kouvorole he bn: phe bsts,\n",
      "Firdd abssetof otit orion buns titalli wolot cavedin, avallthe IV sdorm asp mhe hamoeeer, Fie;,\n",
      "UUVCALA:\n",
      "MEund pin \n",
      "----\n",
      "iter 1000, loss: 82.512553\n",
      "----\n",
      " The yor yolecA shity,\n",
      "\n",
      "ITLMIUTI:\n",
      "\n",
      "MBRCCOUCMBCMURVIlere tfime swimes vas thitn:\n",
      "AM yod thochor- ketadr guhht an:\n",
      "Che afopse ans chiu aw!\n",
      "Thit heit ange.\n",
      "O shof 'd ohe eref\n",
      "Cile sor othis thatiut.\n",
      "y\n",
      "me  \n",
      "----\n",
      "iter 1100, loss: 80.520213\n",
      "----\n",
      " ome man treohens ser thiHoku torl hapst oend soly rohe foa erethe,\n",
      "\n",
      "MARVIUH:\n",
      "uie whad whe!\n",
      "\n",
      "Coe alins yonon, dause aan trest selu Theflosy whedgu fI Oes hecos haouns hiah himatte ur aufande matht yoot \n",
      "----\n",
      "iter 1200, loss: 78.830508\n",
      "----\n",
      " I\n",
      "A'seldeibltS gh hat,\n",
      "AI\n",
      "Whrciss:\n",
      "Om arind, Iangre mey dasg tile lorchave, Co ineg mok arllm:\n",
      ",\n",
      "Fhimnose me pepe one-\n",
      "TO\n",
      "A? Sing\n",
      "Thaur;\n",
      "'t mapuved leroselo faus ond cof woukt o th orcy a'golve bircif \n",
      "----\n",
      "iter 1300, loss: 77.395801\n",
      "----\n",
      " on he lorou I irst toueg.\n",
      "ane oun coe ctiqutlmnres slor\n",
      "Aroursr vory gor ncives ay er or Seme noniant; rary,\n",
      "An.\n",
      "\n",
      "wCOAS:\n",
      "I,\n",
      "Wan tou. non yon anfs etos om mar acoure cilk womfiund meet at I whe lole hi \n",
      "----\n",
      "iter 1400, loss: 76.147195\n",
      "----\n",
      " eon aer at wan'ts glene thook t teedind slrle h,\n",
      "y ondinr it y; Mot welinof roul ond thes.\n",
      "\n",
      "ARUNRTIUSS:\n",
      "I y f ars?\n",
      "ae baubne, wotiims wof snond fed. oode in, foremt ae\n",
      "telnd?\n",
      "Bsar yepey rshan fot and  \n",
      "----\n",
      "iter 1500, loss: 74.801441\n",
      "----\n",
      " hou,\n",
      "Cefgcao'y on was hithir irchesu houtholcovaume onorr mociill wafio herg; hiursenon  on cout sit wass, wot kechee noutand sired ialcns,\n",
      "S sheaks yo!,\n",
      "ya tsis tharreshre medil pbketakes yowls a she \n",
      "----\n",
      "iter 1600, loss: 73.764867\n",
      "----\n",
      " aWant\n",
      "Ble hamed thirpinghe rorlre tefs in.\n",
      "ires homiiosin Iir,\n",
      "\n",
      "Fwle yosoute yotirtmeilme heoon Rhon sow bern-styeer mougey ant lefer grat,\n",
      "IUNNUGABINea tha waryol lod att\n",
      "\n",
      "ENESiralws ake an bid.H Ro! \n",
      "----\n",
      "iter 1700, loss: 72.639567\n",
      "----\n",
      " le pcaln ow ponrsU prnest the goted beanow thilin! bvir\n",
      "\n",
      "ORUDC'Thaz. orr tou tionlls the rotime tou hie,t Iicety are o mWevevhesnd the merveim mliuf ad pLod wet oot woce! gol fozen arvees sir,-E-wine  \n",
      "----\n",
      "iter 1800, loss: 71.787455\n",
      "----\n",
      "  sou\n",
      "Hole suf mlerende\n",
      "pivens wes,\n",
      "Witot solid hedamand Mnes tivew mo susut sKas pironoldgt bt jifl turesit he feond wire; ephye ass wull 'chl therss colory memiw to Ale\n",
      "'stet Matif, tns iprsiu ano wi \n",
      "----\n",
      "iter 1900, loss: 70.704119\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by)\n",
    "smooth_loss = -np.log(1.0 / vocab_size) * seq_length\n",
    "while True:\n",
    "    if p + seq_length + 1 > len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "    \n",
    "      # sample from the model now and then\n",
    "    if n % 100 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print '----\\n %s \\n----' % (txt, )\n",
    "    \n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 100 == 0: print 'iter %d, loss: %f' % (n, smooth_loss) # print progress\n",
    "\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "        \n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter \n",
    "    \n",
    "    if n >= 2000: break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
